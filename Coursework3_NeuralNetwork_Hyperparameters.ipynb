{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report by Ida Johanne Austad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST & Multilayer Perceptron - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is a response to the coursework for week 5 in Machine Learning and Statistics 2 - on Neural Networks. The aim of the report is to show understanding of what a Multilayer Perceptron is, how to build one and how the different hyperparameters affect the performance of a Multilayer Perceptron. This will be done using the MNIST dataset, where the aim is to correctly classify handwritten numbers from 0-9. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report will first give a short introduction to what a Multi-Layer Perceptron is, the purpose of hyperparameter tuning and a short introduction to Gradient Descent. Secondly, a baseline model will be created, trained and evaluated for the purpose of comparison as we go on to tweak the different hyperparameters. This baseline model is equivalent to one of the models presented in the material from class, without dropout, as this will be one of the hyperparameters investigated. After the baseline model has been created the report will investigate the effect of one hyperparameter at a time, while keeping all others the same as in the baseline model. At the end a short discussion and conclusion will summarize what has been learnt and how one would move on with further development of the network.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Multilayer Perceptron and what is the purpose of hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multilayer Perceptron is an Artificial Neural Network inspired by the way the brain works. It comprises of one or more hidden layers of units (aka neurons, like in the brain), in addition to the input and output layers. Between the units are directed connections with weights. By passing samples of observations through the network we allow the network to understand the particularities and commonalities of the observations, and will as such learn the \"nature\" (features) of what it is observing. This is done incrementally by adjusting the connection weights and biases (i.e. the networks parameters) in each iteration and in the end land on an estimated value for all these.  This can then, for instance, be used to classify different \"cases\" apart - such as hand written numbers in this coursework. Since there are several layers of units in MLP, it is a Deep Learning technique. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters decide the structure of the network (e.g. number of hidden layers and number of units) and determines how the network will learn (e.g. Learning Rate, Momentum). The value which these hyperparameters take will have a great effect on the performance of the network - i.e. its ability to generalize and learn \"the rules\" from a training sample and apply this \"knowledge\" to unseen data. In this coursework we will investigate how well a MLP will be able to recognize handwritten numbers between 0 and 9, after some training, using different values of a selection of hyperparameters. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hyperparameters will be investigated in this report:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Number of hidden layers <br> \n",
    " - Number of units per layer <br> \n",
    " - Dropout <br> \n",
    " - Learning rate <br> \n",
    " - Momentum <br> \n",
    " - Batch size <br> \n",
    " - Optimization algorithm <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note I: Here, we will only do one simulation with one random seed per hyperparameter. Weight initialization is one way to tune the implementation of a neural network, but this will not be subject to investigation in this report. Several of the weight initialization alternatives provided by Keras are affected by the random seed, including the default initializer \"glorot_uniform\" for Dense layers which is used in this report [10, 11]. The weights direct the starting conditions for the simulation and as such the result (accuracy) be obtain for each simulation. In reality one would run the code several times with different random seeds to ensure that the obtained accuracy was not just \"luck\", but that the implementation has a stable performance across different starting conditions (weights) [12]. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note II: The verbose output, showing the result of each epoch, has been left out of this report to avoid it becoming to lenghty. Still, end results such as test accuracy is of course included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent - some background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the baseline model uses the optimizing algorithm Mini Batch Stochastic Gradient Descent and several of the hyperparameters to be tuned are passed as arguments to adjust how this optimizer works, it is valuable to give a it a short intoduction to Gradient Descent here - so that the later discussions have sufficient background and context. In short, Gradient Descent is an optimization algorithm which will move through our error space - drawn up by the errors/loss given by different combinations of the parameters (weights and bias) our network is trying to estimate - to try to find the location in this space with the lowest value; a minima - the combination of parameter values which gives the lowest error/loss. A gradient can be defined as “... how much the output of a function changes if you change the inputs a little bit.” [13], and thus a Gradient Descent algorithm uses the gradient, or slope if you want, to find which direction to move - which allows us to reduce the error/loss the most. That would be the direction with the largest negative gradient/slope. The process is often compared with climbing down a mountain into the vally, preferrably the deepest vally. There are different variations of Gradient Descent which use different amounts of the training data before the update parameters. The version used in the baseline model is Mini Batch Stochastic Gradient Descent. The sections investigating Batch Size will get further into the different versions and the meaning of batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for baseline model and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the hyperparameters to be used in the baseline model, loads and prepares the dataset (MNIST) to be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting baseline model parameters used for training \n",
    "# will be changed during the hyperparameter tuning\n",
    "\n",
    "# number of times the entire training dataset \n",
    "#is shown to the network during training\n",
    "N_EPOCH = 20 \n",
    "\n",
    "# number of training examples in one \n",
    "#backward/forward pass (before updating weights)\n",
    "BATCH_SIZE = 128 \n",
    "\n",
    "VERBOSE = 1\n",
    "N_CLASSES = 10   # number of output classes (numbers 0-9)\n",
    "OPTIMIZER = SGD() # Stochastic gradient descent optimiser\n",
    "N_HIDDEN = 128 # number of hidden units\n",
    "VALIDATION_SPLIT= 0.2 # proportion of data used for validation\n",
    "\n",
    "#28x28 pixels reshaped in a vector of 784 pixels\n",
    "RESHAPED = 784\n",
    "\n",
    "# random seed \n",
    "np.random.seed(222) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset and split into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset and split into training and test set\n",
    "(input_X_train, output_Y_train), (input_X_test, output_Y_test) \n",
    "                                        = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape input data to a linear vector of 784 and normalise input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 60000 images for training, 10000 for validation test\n",
    "input_X_train = input_X_train.reshape(60000, RESHAPED)\n",
    "input_X_test = input_X_test.reshape(10000, RESHAPED)\n",
    "input_X_train = input_X_train.astype('float32')\n",
    "input_X_test = input_X_test.astype('float32')\n",
    "\n",
    "# normalisation to 0-1 range \n",
    "input_X_train /= 255\n",
    "input_X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare output data by using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "output_Y_train = np_utils.to_categorical(output_Y_train, N_CLASSES)\n",
    "output_Y_test = np_utils.to_categorical(output_Y_test, N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, train and test baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a multi-layer perceptron equal to one introduced in class to use as baseline model for comparison with variations in hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model object\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "# hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# output layer with 10 units (0-9) and softmax activation\n",
    "model.add(Dense(N_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compilation\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the network\n",
    "history = model.fit(input_X_train, \n",
    "                    output_Y_train, batch_size=BATCH_SIZE, \n",
    "                    epochs=N_EPOCH, verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 32us/step\n",
      "\n",
      "Test score/loss: 0.19407952138632537\n",
      "Test accuracy: 0.9455\n"
     ]
    }
   ],
   "source": [
    "#test the network\n",
    "score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score/loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result we want to compare with - a test loss of 0.194 and 0.9455 in test accuracy. We will through exploring the different hyperparameters below see if we can improve our model and in general learn how the different hyperparameters affect the model's performance. It is considered too extensive to create plots equivalent to the ones above for all the alternative models created in this report. Still, it is important to note that looking at these plots and training vs test performance (for obtained accuracy and loss) evolves across epochs could have been interesting in several of the cases. This could for instance affect decisions such as how many epochs to use or give other insights into the training and test process which can guide the choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network size: Number of hidden layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hidden layers is the number of layers of units in addition to the input and output layer in the network. It is said to be chosen carefully. One reason is that too many layers can result in overfitting (overtraining on the training data, and thus not being able generalize to unseen observations), while to few can result in higher bias and underfitting. Also, it is not necessary to have more layers than the complexity of the problem at hand necessitates, which would mean longer time spent on training than really required. The more complex the problem - the more layers may be desirable [1,8]. Here we will test with 3 and 4 hidden layers, as the baseline model had 2 (and we tested with 1 in the practical session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = [2,3] #in addition to first hidden layer and output layer\n",
    "test_score = []\n",
    "test_accuracy = []    \n",
    "  \n",
    "for i in N_LAYERS:\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Hidden layer 1 which reshapes the input\n",
    "    model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    #loop to add i layers with ReLU activation function\n",
    "    for j in range(i):\n",
    "            model.add(Dense(N_HIDDEN))\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(input_X_train, output_Y_train, \n",
    "                        batch_size=BATCH_SIZE, epochs=N_EPOCH, \n",
    "                        verbose=VERBOSE, \n",
    "                        validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    score = model.evaluate(input_X_test, output_Y_test, \n",
    "                           verbose=VERBOSE)\n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 3 hidden layers gave:\n",
      "Test score/loss 0.1622661118760705\n",
      "Test accuracy: 0.9497\n",
      "\n",
      "Using 4 hidden layers gave:\n",
      "Test score/loss 0.14527234137579798\n",
      "Test accuracy: 0.9564\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(N_LAYERS)):\n",
    "    print(\"\\nUsing \" + str(N_LAYERS[i]+1) + \" hidden layers gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the test accuracy has increased, compared to the baseline model (0.945), for each hidden layer added to the model. Out of the two results above the model with 4 hidden layers is the best, and in a real life strategy this might motivate trying even higher numbers of layers. On the other hand, adding additional layers to a model which is to solve a relatively simple problem such as MNIST, can be considered an expensive way to improve the performance due to the increased complexity of the network structure. Thus, it may be wise to investigate other hyperparameters before deciding to increase the number of layers added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer size: Number of units per hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the number of layers, the number of units per hidden layer has to be chosen carefully to avoid over- or underfitting. Adding more units to the layers can make the model better able to understand the features of the data, but too many can on the other hand learn \"too much\" about the training data, not being able to generelize to unseen data (overfitting).  As for layers, too many units can result in that it takes longer time than necessary to train the network. The more complex the problem the more units may be desirable. There are several rules which guides how many units to choose, and one rule says to choose a number between the number of input units (785 here, 784+1 bias unit) and number of output units (10 here), and this is the motivation for the values chosen below [1,8]. The value was 128 for the baseline model, and is thus not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of units to try per layer\n",
    "N_HIDDEN = [64,256, 512]\n",
    "test_score = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in N_HIDDEN:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1 with i hidden units and ReLu activation function\n",
    "    model.add(Dense(i, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Hidden layer 2 with i hidden units and ReLu activation function\n",
    "    model.add(Dense(i))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "    \n",
    "    #train the network\n",
    "    history = model.fit(input_X_train, \n",
    "                        output_Y_train, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=N_EPOCH, \n",
    "                        verbose=VERBOSE, \n",
    "                        validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    score = model.evaluate(input_X_test, \n",
    "                           output_Y_test, verbose=VERBOSE)\n",
    "\n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 64 units gave:\n",
      "Test score/loss 0.2006501191586256\n",
      "Test accuracy: 0.9414\n",
      "\n",
      "Using 256 units gave:\n",
      "Test score/loss 0.18610936435237527\n",
      "Test accuracy: 0.945\n",
      "\n",
      "Using 512 units gave:\n",
      "Test score/loss 0.17023214867115022\n",
      "Test accuracy: 0.9519\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(N_HIDDEN)):\n",
    "    print(\"\\nUsing \" + str(N_HIDDEN[i]) + \" units gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that one of the models we just trained performed better that our baseline model (0.9455). The model with 512 units per hidden layer, versus 128 in the baseline, gives a better test accuracy. This is as expected as adding more units to the layers was explained in this section's introduction to potentially improve the model's ability to understand the features in the data. As for adding more hidden layers, adding more units to hidden layers may slow down the training of the model and potentially make it unnessecarily complex. Thus, it must be investigated in conjunction with tuning of other parameters. Still, one could have added a few more options in this section - more units per layer - to see if the test accuracy started to decrease again at some point (indicating potential overfitting due to too many units per layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout means that we randomly drop a specified proportion of the units from each layer in each training iteration and in effect end up with a simpler network (only during training, as dropout is not applied in test). It is a method to prevent the model from overfitting. It has the effect of making the training process more noisy and forces the units within a layer to be more attentive as gets \"less\" learning opportunities and cannot depend on all the other units in it's layer in each iteration. It is said that the units learn more \"robust\" features using dropout. Altough dropout often necessitates more iterations, it takes less time per epoch [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the number of hidden units to be the same as in the baseline\n",
    "N_HIDDEN = 128\n",
    "\n",
    "#create vector to hold the dropout percentages we want to try\n",
    "P_DROPOUT = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "test_score = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in P_DROPOUT:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(i))\n",
    "\n",
    "    # Hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(i))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "    \n",
    "    #train the network\n",
    "    history = model.fit(input_X_train, output_Y_train, \n",
    "                        batch_size=BATCH_SIZE, epochs=N_EPOCH, \n",
    "                        verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    score = model.evaluate(input_X_test, \n",
    "                           output_Y_test, verbose=VERBOSE)\n",
    "    \n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout rate 0.1 gave:\n",
      "Test score/loss 0.18116655552238226\n",
      "Test accuracy: 0.9479\n",
      "\n",
      "Using dropout rate 0.2 gave:\n",
      "Test score/loss 0.1852808731086552\n",
      "Test accuracy: 0.9454\n",
      "\n",
      "Using dropout rate 0.3 gave:\n",
      "Test score/loss 0.19091025214716792\n",
      "Test accuracy: 0.9426\n",
      "\n",
      "Using dropout rate 0.4 gave:\n",
      "Test score/loss 0.20369335896596313\n",
      "Test accuracy: 0.9408\n",
      "\n",
      "Using dropout rate 0.5 gave:\n",
      "Test score/loss 0.21960563701242208\n",
      "Test accuracy: 0.9346\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(P_DROPOUT)):\n",
    "    print(\"\\nUsing dropout rate \" + str(P_DROPOUT[i]) + \" gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the results above we see that one of the dropout options, with dropout at 0.1, performed better than our baseline model (0.9455). This can be because we have chosen to drop our rather large proportion of the units - up to 0.5. Still, we can see that the accuracy remains above 0.93 for all options - while the training took less time due to the reduced number of units involved in each iteration. If one where to investigate this further one could see if any values around 0.1 gave an even better accuracy, but for the purpose of this courswork this result is considered sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is the hyperparameter which controls how much we will adjust the weights after each iteration. The lower the value of the learning rate the slower we will move along the error surface towards a minima (can be global or local) - as we take \"smaller steps\", while larger value allows us to take larger steps. While a small learning rate will converge slowly towards the minima, and will as such be less likely to jump past or miss the slope down to a minima, it can take too much time. A learning rate which is too high can make us miss the minima as we \"jump\" past it and can potentially not converge. A low learning rate can also get us stuck in a plateau, if a region in the error surface is flat [4]. The default value used by Keras' SDG is 0.01, so we shall try a few alternatives [7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify vector of learning rates we will test\n",
    "LEARN_R = [0.001, 0.1,0.2, 0.3, 0.5, 0.6]\n",
    "test_score = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in LEARN_R:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #for each loop we want to use a new value for the learning rate\n",
    "    OPTIMIZER = SGD(lr = i)    \n",
    "\n",
    "    # compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "    \n",
    "    #train the network\n",
    "    history = model.fit(input_X_train, output_Y_train, \n",
    "                        batch_size=BATCH_SIZE, epochs=N_EPOCH, \n",
    "                        verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    #get the score and store them to print after finish\n",
    "    score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using learning rate 0.001 gave:\n",
      "Test score/loss 0.4671427490711212\n",
      "Test accuracy: 0.8762\n",
      "\n",
      "Using learning rate 0.1 gave:\n",
      "Test score/loss 0.08377402248502476\n",
      "Test accuracy: 0.9742\n",
      "\n",
      "Using learning rate 0.2 gave:\n",
      "Test score/loss 0.0771518416537554\n",
      "Test accuracy: 0.9801\n",
      "\n",
      "Using learning rate 0.3 gave:\n",
      "Test score/loss 0.08348359558877837\n",
      "Test accuracy: 0.9794\n",
      "\n",
      "Using learning rate 0.5 gave:\n",
      "Test score/loss 0.09487410609824964\n",
      "Test accuracy: 0.9803\n",
      "\n",
      "Using learning rate 0.6 gave:\n",
      "Test score/loss 0.09507778749285883\n",
      "Test accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(LEARN_R)):\n",
    "    print(\"\\nUsing learning rate \" + str(LEARN_R[i]) + \" gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the reults above we can see that all variations of learning rate except 0.001 (the only one lower than the learning rate used in our baseline model) managed to get a better test accuracy than 0.9455. The best results are given by the model using a learning rate of 0.6 - with an accuracy of 0.9806. As such we can see how important the potential of tuning the learning rate is, and that we in a real scenario could have been interested in investigating the results of using learning rates higher and/or around 0.6 - for further improvement. Still, setting it too high comes with risks - as discussed before. That is where the hyperparameter Momentum can help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum takes values from 0 to 1 and directs how much we should let our previous weight updates affect the current weight update. Choosing a good momentum can thus help us reduce the risks we discussed related to choosing a too low/too high learning rate (too small - too slow training, too large - passes the minima and the potential of never converging). Momentum has the effect of adapting the learning rate to the \"territory\" that we are experiencing in the error space. It speeds up the training by allowing to move faster (larger updates to weights) in the beginning and/or when the slope is steep, but as the slope changes direction it will reduce the weight updates and thus make it less likely that one will miss a minima and more likely to converge [9]. The default value used in Keras SGD is 0.0, thus a few other values will be tried [7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify vector of momentums we want to test out\n",
    "MOMENTUM = [0.2, 0.4,0.6, 0.8]\n",
    "test_score = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in MOMENTUM:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #for each loop we want to use a new value for the momentum\n",
    "    OPTIMIZER = SGD(momentum = i)    \n",
    "\n",
    "    # compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "    \n",
    "    #train the network\n",
    "    history = model.fit(input_X_train, output_Y_train, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=N_EPOCH, verbose=VERBOSE, \n",
    "                        validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    #get the score and store them to print after finish\n",
    "    score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Momentum 0.2 gave:\n",
      "Test score/loss 0.16845399807095526\n",
      "Test accuracy: 0.9507\n",
      "\n",
      "Using Momentum 0.4 gave:\n",
      "Test score/loss 0.1414313634209335\n",
      "Test accuracy: 0.9573\n",
      "\n",
      "Using Momentum 0.6 gave:\n",
      "Test score/loss 0.11963009840995073\n",
      "Test accuracy: 0.9643\n",
      "\n",
      "Using Momentum 0.8 gave:\n",
      "Test score/loss 0.08930754054784774\n",
      "Test accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(MOMENTUM)):\n",
    "    print(\"\\nUsing Momentum \" + str(MOMENTUM[i]) + \" gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above we can see that all the variations of momentum performs better than our baseline (0.9455). The best result is given with a momentum of 0.8, with a test accuracy of 0.9742. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is a hyperparameter (when using the Gradient Descent optimization algorithm) which specifies the number of samples shown to the network before parameters are updated. In our baseline model and the previous variations we used Stochastic Gradient Descent (SDG), with a batch size of 128, as our optimizer. The default batch size with SDG is 1. That is, we show the network 1 sample before it updates its parameter value. If one were to define a batch as the entire training dataset, the optimizing method is called Batch Gradient Descent [5]. Smaller batch sizes has the effect that it presents the network with more noise and as such can allow the network to become better at generalization - and thus reduce likelihood overfitting. Also, smaller batch sizes are easier to fit into memory when using a GPU, and can as such speed up the training [6]. <br>\n",
    "\n",
    "In this section we will see what happens when we change the batch size to values: 1 < Batch Size < size of entire training set. This version of Gradient Descent is known as Mini-Batch Gradient Descent, and has been used in the previous models as well. Here we test with batch sizes which are suggested by practicians, excluding the one used in the baseline model (=128) [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset optimiser to the same as the baseline\n",
    "OPTIMIZER = SGD()\n",
    "\n",
    "#define a vector of batch sizes to test\n",
    "BATCH_SIZE = [32,64,256, 512]\n",
    "test_score = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in BATCH_SIZE:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "    #train the network\n",
    "    history = model.fit(input_X_train, output_Y_train, \n",
    "                        batch_size=i, epochs=N_EPOCH, \n",
    "                        verbose=VERBOSE, \n",
    "                        validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    score = model.evaluate(input_X_test, output_Y_test, \n",
    "                           verbose=VERBOSE)\n",
    "\n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Batch Size 32 gave:\n",
      "Test score/loss 0.09763096881974488\n",
      "Test accuracy: 0.9711\n",
      "\n",
      "Using Batch Size 64 gave:\n",
      "Test score/loss 0.13461268049366773\n",
      "Test accuracy: 0.9585\n",
      "\n",
      "Using Batch Size 256 gave:\n",
      "Test score/loss 0.2559405641555786\n",
      "Test accuracy: 0.9278\n",
      "\n",
      "Using Batch Size 512 gave:\n",
      "Test score/loss 0.3147944088339806\n",
      "Test accuracy: 0.9121\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(BATCH_SIZE)):\n",
    "    print(\"\\nUsing Batch Size \" + str(BATCH_SIZE[i]) + \" gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can se that the two options which specify batch size smaller than the baseline model, also gives better results. The best test accuracy is given by using batch size of 32 with a test accuracy of 0.9711. This is in line with practitioners statments saying that batch sizes of 32 and lower has shown to perform the best [6]. As such, one could have been interested in testing even lower batch sizes to see the effect on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the setting of Neural Networks, as described in the introduction to Gradient Descent as an example, the optimization algorithm helps us to minimze of our objective function - which is the loss/error in the training process, and thus maximise the training accuracy. There are different ways of navigating the error space, which has thus come to be different optimization algorithms. We have already seen two adjustments which can be made to SDG - namely the size of our steps in the error space (learning rate) and how much we take into account previous steps (momentum). These where both parameters passed to the optimizer used in the baseline model - (Mini Batch) Stochastic Gradient Descent. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will test three alternative optimization algorithms and evaluate their performance. All try to handle the potential risks and issues faced by Gradient Descent, as mentioned earlier; potential of slow convergence with lowe learning rate, potential of missing minimia or diverging in case of large learning rate, issues with finding minima in highly non-convex error spaces (many local minimas) and potental of getting trapped in flat regions (saddle points). We already looked at the hyperparameter Momentum, which helped mitigate these risks, and which has thus been integrated into these algorithms. However, one challenge with Momentum is that is does not necessarialy slow down the weight updates as it gets close to the minima - if its speed (and as such size of weight updates) is too large. Thus, one can still miss a minima when using momentum [14]. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Adagrad: This optimization algorithm allows the learning rate to adapt based on the parameters. For parameters which are updated often, the learning rate will be smaller. For parameters which are updated more seldom it will make larger updates. As such it takes all previous updates (gradients) of each parameter into account and allows for an adaptive learning rate. This has the benefit that one does not have to specify a specific learning rate, and the learning rate will adapt automtically - as when specifying a momentum [14]. A disadvantage with using Adagrad is that it faces a decaying learning rate. Due the fact that it takes all past gradients equally into account when estimating the learning rate per parameter - the learning rate will eventually become very small, and the model will not learn anymore. The result is that convergence and thus training can take too much time. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ADAM: Adaptive Moment Estimation (ADAM) also uses adaptive learning rates for each parameter. But, by computing the learning rates for the parameters from both the first and second moments of the previous gradients it obtain even better results, and avoids the issue with decreasing learning rate. Using ADAM, both the means of the previous moments and the uncentered variance of the previous moments are taken into account when adapting the parameters' learning rates [15]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset batch size to same as baseline\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#define vector with optimization algorithms to try\n",
    "OPTIMIZER = [\"Adagrad\", \"Adam\"]\n",
    "test_score = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in OPTIMIZER:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "    model.add(Dense(N_HIDDEN))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # output layer with 10 units and softmax activation\n",
    "    model.add(Dense(N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # compilation with optimization algorithm i\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=i, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    #train the network\n",
    "    history = model.fit(input_X_train, output_Y_train, \n",
    "                        batch_size=BATCH_SIZE, epochs=N_EPOCH, \n",
    "                        verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    #get the score and store them to print after finish\n",
    "    score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "    test_score.append(score[0])\n",
    "    test_accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Optimizer Adagrad gave:\n",
      "Test score/loss 0.07125321100392612\n",
      "Test accuracy: 0.9769\n",
      "\n",
      "Using Optimizer Adam gave:\n",
      "Test score/loss 0.09775170024595554\n",
      "Test accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(OPTIMIZER)):\n",
    "    print(\"\\nUsing Optimizer \" + str(OPTIMIZER[i]) + \" gave:\")\n",
    "    print(\"Test score/loss\", test_score[i])\n",
    "    print(\"Test accuracy:\", test_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results summarized - test accuracy: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Baseline model: 0.9455 <br>\n",
    " - 4 hidden layers: 0.9564 <br>\n",
    " - 512 units per hidden layer: 0.9519 <br>\n",
    " - Dropout proportion 0.1: 0.9479 <br>\n",
    " - Learning rate 0.6: 0.9806 <br>\n",
    " - Momentum 0.8: 0.9742 <br>\n",
    " - Batch size 32: 0.9711 <br>\n",
    " - ADAM: 0.9785 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we see that it was the adjustment of the hyperparameter learning rate to 0.6 which gave the highest test accuracy and thus the largest improvement compared to the baseline model. However, we have investigated and learned how all the different hyperparameters affect how a Neural Network model can perform, and that they can all have their contributions and play a role. Moreover, it is important to mention that one would usually tune the hyperparameters together - looking for the best combinations of hyperparameters.  This is because the hyperparameters can affect each other, and should as such be investigated together. Thus, saying that we would move forward with a combination of the hyperparameters presented in this list would not be advisable - one should see how they perform together - with different values. Lastly, one must take the problem at hand into account when making these decisions and take into account additional considerations  - not just test accuracy, such as time needed for training, type and characteristics of data and problem etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are hyperparameters? And how to tune the Hyperparameters in a Deep Neural Network? https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a <br>\n",
    "2. Improving Neural Networks – Hyperparameter Tuning, Regularization, and More  https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/ <br>\n",
    "3. A Gentle Introduction to Dropout for Regularizing Deep Neural Networks https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/ <br>\n",
    "4. Understanding Learning Rates and How It Improves Performance in Deep Learning https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10 <br>\n",
    "5. What is the Difference Between a Batch and an Epoch in a Neural Network? https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/ <br>\n",
    "6. How to Control the Speed and Stability of Training Neural Networks With Gradient Descent Batch Size https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/ <br> \n",
    "7. Keras Documentation - Optimizers https://keras.io/optimizers/ <br>\n",
    "8. How to choose the number of hidden layers and nodes in a feedforward neural network? https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "9. Neural Network Momentum https://jamesmccaffrey.wordpress.com/2017/06/06/neural-network-momentum/\n",
    "10. Keras Documentation - Initializers https://keras.io/initializers/\n",
    "11. Keras Documentation - Core Layers https://keras.io/layers/core/\n",
    "12. Where do 'random seeds' get used in deep neural networks? https://ai.stackexchange.com/questions/6240/where-do-random-seeds-get-used-in-deep-neural-networks\n",
    "13. Gradient Descent in a Nutshell https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0\n",
    "14. Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f\n",
    "15. Gentle Introduction to the Adam Optimization Algorithm for Deep Learning https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
